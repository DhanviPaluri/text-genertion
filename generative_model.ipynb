{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4715e447",
   "metadata": {},
   "source": [
    "# CodTech Internship Task 4 - Generative Text Model\n",
    "This notebook demonstrates text generation using two approaches:\n",
    "- Pretrained GPT-2 (via Hugging Face Transformers)\n",
    "- Custom LSTM model (using TensorFlow/Keras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6487885",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers torch tensorflow tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a7c44ee",
   "metadata": {},
   "source": [
    "## üß† GPT-2 Based Text Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba3629f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "import torch\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "model.eval()\n",
    "\n",
    "prompt = \"The future of artificial intelligence is\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "outputs = model.generate(**inputs, max_length=100, num_return_sequences=1)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "340995e0",
   "metadata": {},
   "source": [
    "## üîÅ LSTM Based Text Generation (Simple Example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d82e1277",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "text = \"hello world hello world hello world\"\n",
    "chars = sorted(list(set(text)))\n",
    "char2idx = {u:i for i, u in enumerate(chars)}\n",
    "idx2char = np.array(chars)\n",
    "text_as_int = np.array([char2idx[c] for c in text])\n",
    "\n",
    "seq_length = 5\n",
    "examples_per_epoch = len(text)//(seq_length+1)\n",
    "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
    "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n",
    "\n",
    "def split_input_target(chunk):\n",
    "    input_text = chunk[:-1]\n",
    "    target_text = chunk[1:]\n",
    "    return input_text, target_text\n",
    "\n",
    "dataset = sequences.map(split_input_target).batch(1, drop_remainder=True)\n",
    "\n",
    "vocab_size = len(chars)\n",
    "embedding_dim = 8\n",
    "rnn_units = 64\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim, batch_input_shape=[1, None]),\n",
    "    tf.keras.layers.LSTM(rnn_units, return_sequences=True, stateful=True, recurrent_initializer='glorot_uniform'),\n",
    "    tf.keras.layers.Dense(vocab_size)\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss=lambda labels, logits: tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True))\n",
    "\n",
    "model.fit(dataset, epochs=10)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b52fe0eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Text generation from trained LSTM\n",
    "input_eval = [char2idx[s] for s in \"hello\"]\n",
    "input_eval = tf.expand_dims(input_eval, 0)\n",
    "text_generated = []\n",
    "temperature = 1.0\n",
    "\n",
    "model.reset_states()\n",
    "for i in range(20):\n",
    "    predictions = model(input_eval)\n",
    "    predictions = tf.squeeze(predictions, 0)\n",
    "    predictions = predictions / temperature\n",
    "    predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
    "    input_eval = tf.expand_dims([predicted_id], 0)\n",
    "    text_generated.append(idx2char[predicted_id])\n",
    "\n",
    "print(\"Generated:\", \"hello\" + ''.join(text_generated))\n",
    "    "
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
